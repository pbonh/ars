---
ollama_linux_installer: "https://ollama.com/install.sh"
ollama_linux_installer_rocm: "https://ollama.com/download/ollama-linux-amd64-rocm.tgz"
ollama_installer_download_path: "{{ dot_download_dir }}/ollama-installer.sh"
ollama_installer_download_rocm: "ollama-linux-amd64-rocm"
ollama_installer_download_path_rocm: "{{ dot_download_dir }}/{{ ollama_installer_download_rocm }}.tgz"
ollama_installer_path_rocm: "/usr"
ollama_macos_installer: "https://ollama.com/download/Ollama-darwin.zip"
ollama_macos_installer_download_path: "{{ dot_download_dir }}/Ollama-darwin"
ollama_macos_app_name: "Ollama.app"
ollama_macos_app: "{{ ollama_macos_installer_download_path }}/{{ ollama_macos_app_name }}"
ollama_macos_app_exe: "/Applications/{{ ollama_macos_app_name }}"
ollama_cmd: "ollama"
ollama_exe: "/usr/local/bin/{{ ollama_cmd }}"
ollama_service_filename: "ollama.service"
ollama_service_install_location: "/etc/systemd/system/{{ ollama_service_filename }}"
rocm_support: false
code2prompt_config: "{{ xdg_config_dir }}/code2prompt"
code2prompt_template_dir: "{{ code2prompt_config }}/templates"
code2prompt_templates:
  r1:
    name: "r1"
    template: |
      1. Goals

      <!--
      Explain the main objective of your prompt or query. 
      - What problem are you trying to solve?
      - What kind of output or insight do you need?
      - Are you asking for a list, a summary, an analysis, or a plan of action?

      Example:
      "Analyze the provided user feedback and suggest three ways to improve product usability."
      -->[Replace this text with your specific goals or objectives.]

      2. Return Format

      <!--
      Define exactly how you want the LLM to present its answer. 
      - Should it be a numbered list, bullet points, a short paragraph, or code snippet?
      - Are there any stylistic guidelines to follow (e.g., no first-person language)?
      - Is there a specific data format like JSON or XML?

      Example:
      "Please return your answer as a JSON object with fields 'summary' and 'recommendations'."
      -->[Replace this text with instructions for the desired format of the response.]

      3. Warnings

      <!--
      List any important caveats, constraints, or sensitive information. 
      - Do you need to remind the LLM about confidentiality or disclaimers?
      - Are there any topics or details to avoid?
      - Are there any factual or ethical considerations?

      Example:
      "Do not share any personal user data. Avoid speculation about user identities."
      -->[Replace this text with important warnings or guidelines for the LLM.]

      4. Context Dump

      <!--
      Provide all the background information that the LLM needs to generate its response. 
      - This can include prior conversation transcripts, relevant facts, or any additional resources.
      - More context generally helps the LLM produce a more accurate and relevant answer.

      Example:
      "Userâ€™s previous conversation logs, relevant product requirements, existing code snippets, design guidelines, etc."
      -->[Replace this text with your detailed context or references.]
